{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['Diabetes_binary'].value_counts())\n",
    "\n",
    "offline_df, online_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['Diabetes_binary']\n",
    ")\n",
    "\n",
    "offline_df.to_csv(\"offline.csv\", index=False)\n",
    "online_df.to_csv(\"online.csv\", index=False)\n",
    "\n",
    "print(f\"\\nOffline dataset: {len(offline_df)} records\")\n",
    "print(f\"Online dataset: {len(online_df)} records\")\n",
    "print(\"\\nFiles saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiabetesOfflineOptimized\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"offline.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df = df.cache()\n",
    "\n",
    "print(f\"Dataset shape: {df.count()} rows x {len(df.columns)} columns\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "df.groupBy(\"Diabetes_binary\").count().show()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "label_col = \"Diabetes_binary\"\n",
    "feature_cols = [c for c in df.columns if c != label_col]\n",
    "\n",
    "print(f\"Features ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_vec\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vec\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "print(\"\\nFeature engineering stages created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col, maxIter=100)\n",
    "lr_pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "lr_params = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Logistic Regression: {len(lr_params)} hyperparameter combinations\")\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col, seed=42)\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "rf_params = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Random Forest: {len(rf_params)} hyperparameter combinations\")\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=label_col, seed=42)\n",
    "dt_pipeline = Pipeline(stages=[assembler, scaler, dt])\n",
    "dt_params = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Decision Tree: {len(dt_params)} hyperparameter combinations\")\n",
    "\n",
    "model_configs = [\n",
    "    (\"Logistic Regression\", lr_pipeline, lr_params),\n",
    "    (\"Random Forest\", rf_pipeline, rf_params),\n",
    "    (\"Decision Tree\", dt_pipeline, dt_params)\n",
    "]\n",
    "\n",
    "print(\"\\nAll models configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "import time\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "results = []\n",
    "best_model = None\n",
    "best_name = \"\"\n",
    "best_f1 = 0.0\n",
    "\n",
    "for name, pipeline, param_grid in model_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=f1_evaluator,\n",
    "        numFolds=5,\n",
    "        parallelism=2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    cv_model = cv.fit(df)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    predictions = cv_model.bestModel.transform(df)\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    accuracy = acc_evaluator.evaluate(predictions)\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {auc:.4f}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"time\": training_time\n",
    "    })\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = cv_model.bestModel\n",
    "        best_name = name\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"F1 Score:  {best_f1:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "model_name = best_name.lower().replace(\" \", \"_\")\n",
    "model_path = f\"models/best_{model_name}\"\n",
    "\n",
    "best_model.write().overwrite().save(model_path)\n",
    "\n",
    "print(f\"\\n✓ Model saved successfully!\")\n",
    "print(f\"  Path: {model_path}\")\n",
    "print(f\"  Model: {best_name}\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "sample = df.limit(10)\n",
    "predictions = loaded_model.transform(sample)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\"Diabetes_binary\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
