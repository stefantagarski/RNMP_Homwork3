{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 253680\n",
      "\n",
      "Class distribution:\n",
      "Diabetes_binary\n",
      "0.0    218334\n",
      "1.0     35346\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Offline dataset: 202944 records\n",
      "Online dataset: 50736 records\n",
      "\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['Diabetes_binary'].value_counts())\n",
    "\n",
    "offline_df, online_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['Diabetes_binary']\n",
    ")\n",
    "\n",
    "offline_df.to_csv(\"offline.csv\", index=False)\n",
    "online_df.to_csv(\"online.csv\", index=False)\n",
    "\n",
    "print(f\"\\nOffline dataset: {len(offline_df)} records\")\n",
    "print(f\"Online dataset: {len(online_df)} records\")\n",
    "print(\"\\nFiles saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/11 21:01:38 WARN Utils: Your hostname, stefan-Aspire-7, resolves to a loopback address: 127.0.1.1; using 192.168.100.73 instead (on interface wlp4s0)\n",
      "26/01/11 21:01:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 21:01:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.1\n",
      "Spark session created successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiabetesOfflineOptimized\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 202944 rows x 22 columns\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- Diabetes_binary: double (nullable = true)\n",
      " |-- HighBP: double (nullable = true)\n",
      " |-- HighChol: double (nullable = true)\n",
      " |-- CholCheck: double (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Smoker: double (nullable = true)\n",
      " |-- Stroke: double (nullable = true)\n",
      " |-- HeartDiseaseorAttack: double (nullable = true)\n",
      " |-- PhysActivity: double (nullable = true)\n",
      " |-- Fruits: double (nullable = true)\n",
      " |-- Veggies: double (nullable = true)\n",
      " |-- HvyAlcoholConsump: double (nullable = true)\n",
      " |-- AnyHealthcare: double (nullable = true)\n",
      " |-- NoDocbcCost: double (nullable = true)\n",
      " |-- GenHlth: double (nullable = true)\n",
      " |-- MentHlth: double (nullable = true)\n",
      " |-- PhysHlth: double (nullable = true)\n",
      " |-- DiffWalk: double (nullable = true)\n",
      " |-- Sex: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Education: double (nullable = true)\n",
      " |-- Income: double (nullable = true)\n",
      "\n",
      "\n",
      "Class distribution:\n",
      "+---------------+------+\n",
      "|Diabetes_binary| count|\n",
      "+---------------+------+\n",
      "|            1.0| 28277|\n",
      "|            0.0|174667|\n",
      "+---------------+------+\n",
      "\n",
      "\n",
      "First 5 rows:\n",
      "+---------------+------+--------+---------+----+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+----+---------+------+\n",
      "|Diabetes_binary|HighBP|HighChol|CholCheck| BMI|Smoker|Stroke|HeartDiseaseorAttack|PhysActivity|Fruits|Veggies|HvyAlcoholConsump|AnyHealthcare|NoDocbcCost|GenHlth|MentHlth|PhysHlth|DiffWalk|Sex| Age|Education|Income|\n",
      "+---------------+------+--------+---------+----+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+----+---------+------+\n",
      "|            0.0|   0.0|     0.0|      1.0|28.0|   1.0|   0.0|                 0.0|         1.0|   1.0|    1.0|              0.0|          1.0|        0.0|    2.0|     0.0|     0.0|     0.0|1.0| 2.0|      4.0|   5.0|\n",
      "|            0.0|   1.0|     0.0|      1.0|23.0|   1.0|   0.0|                 0.0|         1.0|   1.0|    1.0|              0.0|          1.0|        0.0|    2.0|     0.0|     0.0|     0.0|1.0|13.0|      4.0|   7.0|\n",
      "|            0.0|   1.0|     1.0|      1.0|29.0|   0.0|   0.0|                 0.0|         1.0|   1.0|    1.0|              0.0|          1.0|        0.0|    1.0|     0.0|     0.0|     0.0|1.0| 9.0|      6.0|   8.0|\n",
      "|            0.0|   1.0|     1.0|      1.0|39.0|   0.0|   0.0|                 0.0|         0.0|   0.0|    0.0|              0.0|          1.0|        0.0|    4.0|     0.0|     0.0|     0.0|1.0| 7.0|      4.0|   7.0|\n",
      "|            0.0|   0.0|     1.0|      1.0|16.0|   1.0|   0.0|                 0.0|         1.0|   1.0|    1.0|              0.0|          1.0|        1.0|    5.0|    30.0|    30.0|     1.0|0.0| 7.0|      5.0|   1.0|\n",
      "+---------------+------+--------+---------+----+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+----+---------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"offline.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df = df.cache()\n",
    "\n",
    "print(f\"Dataset shape: {df.count()} rows x {len(df.columns)} columns\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "df.groupBy(\"Diabetes_binary\").count().show()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (21):\n",
      " 1. HighBP\n",
      " 2. HighChol\n",
      " 3. CholCheck\n",
      " 4. BMI\n",
      " 5. Smoker\n",
      " 6. Stroke\n",
      " 7. HeartDiseaseorAttack\n",
      " 8. PhysActivity\n",
      " 9. Fruits\n",
      "10. Veggies\n",
      "11. HvyAlcoholConsump\n",
      "12. AnyHealthcare\n",
      "13. NoDocbcCost\n",
      "14. GenHlth\n",
      "15. MentHlth\n",
      "16. PhysHlth\n",
      "17. DiffWalk\n",
      "18. Sex\n",
      "19. Age\n",
      "20. Education\n",
      "21. Income\n",
      "\n",
      "Feature engineering stages created!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "label_col = \"Diabetes_binary\"\n",
    "feature_cols = [c for c in df.columns if c != label_col]\n",
    "\n",
    "print(f\"Features ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_vec\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vec\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "print(\"\\nFeature engineering stages created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 9 hyperparameter combinations\n",
      "Random Forest: 18 hyperparameter combinations\n",
      "Decision Tree: 24 hyperparameter combinations\n",
      "\n",
      "All models configured!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col, maxIter=100)\n",
    "lr_pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "lr_params = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Logistic Regression: {len(lr_params)} hyperparameter combinations\")\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col, seed=42)\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "rf_params = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Random Forest: {len(rf_params)} hyperparameter combinations\")\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=label_col, seed=42)\n",
    "dt_pipeline = Pipeline(stages=[assembler, scaler, dt])\n",
    "dt_params = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Decision Tree: {len(dt_params)} hyperparameter combinations\")\n",
    "\n",
    "model_configs = [\n",
    "    (\"Logistic Regression\", lr_pipeline, lr_params),\n",
    "    (\"Random Forest\", rf_pipeline, rf_params),\n",
    "    (\"Decision Tree\", dt_pipeline, dt_params)\n",
    "]\n",
    "\n",
    "print(\"\\nAll models configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: Logistic Regression\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 21:02:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "26/01/11 21:02:14 WARN BlockManager: Block rdd_320_0 already exists on this machine; not re-adding it\n",
      "26/01/11 21:02:14 WARN BlockManager: Block rdd_320_2 already exists on this machine; not re-adding it\n",
      "26/01/11 21:02:14 WARN BlockManager: Block rdd_320_1 already exists on this machine; not re-adding it\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "import time\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "results = []\n",
    "best_model = None\n",
    "best_name = \"\"\n",
    "best_f1 = 0.0\n",
    "\n",
    "for name, pipeline, param_grid in model_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=f1_evaluator,\n",
    "        numFolds=5,\n",
    "        parallelism=2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    cv_model = cv.fit(df)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    predictions = cv_model.bestModel.transform(df)\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    accuracy = acc_evaluator.evaluate(predictions)\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {auc:.4f}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"time\": training_time\n",
    "    })\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = cv_model.bestModel\n",
    "        best_name = name\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"F1 Score:  {best_f1:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "model_name = best_name.lower().replace(\" \", \"_\")\n",
    "model_path = f\"models/best_{model_name}\"\n",
    "\n",
    "best_model.write().overwrite().save(model_path)\n",
    "\n",
    "print(f\"\\n✓ Model saved successfully!\")\n",
    "print(f\"  Path: {model_path}\")\n",
    "print(f\"  Model: {best_name}\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "sample = df.limit(10)\n",
    "predictions = loaded_model.transform(sample)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\"Diabetes_binary\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
